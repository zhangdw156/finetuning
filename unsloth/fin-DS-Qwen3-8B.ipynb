{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4f8cee-de9d-4c43-8f54-b02599eec07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Qwen3 patching. Transformers: 4.52.4. vLLM: 0.9.1.\n",
      "   \\\\   /|    NVIDIA H20. Num GPUs = 8. Max memory: 94.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading /data/download-model/DeepSeek-R1-0528-Qwen3-8B with actual GPU utilization = 19.11%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 94.99 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 2.82 GB. Also swap space = 6 GB.\n",
      "INFO 06-17 03:20:04 [config.py:823] This model supports multiple tasks: {'embed', 'reward', 'score', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-17 03:20:04 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 03:20:05 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/download-model/DeepSeek-R1-0528-Qwen3-8B', speculative_config=None, tokenizer='/data/download-model/DeepSeek-R1-0528-Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/download-model/DeepSeek-R1-0528-Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-17 03:20:05 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f337d7d5ff0>\n",
      "INFO 06-17 03:20:05 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-17 03:20:05 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-17 03:20:05 [gpu_model_runner.py:1595] Starting to load model /data/download-model/DeepSeek-R1-0528-Qwen3-8B...\n",
      "INFO 06-17 03:20:06 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 06-17 03:20:06 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-17 03:20:06 [bitsandbytes_loader.py:454] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db353d931f7247f89db5649682441ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 03:20:10 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 06-17 03:20:10 [gpu_model_runner.py:1624] Model loading took 5.8260 GiB and 3.915533 seconds\n",
      "INFO 06-17 03:20:21 [backends.py:462] Using cache directory: /home/adminad/.cache/vllm/torch_compile_cache/ed299ab4c5/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-17 03:20:21 [backends.py:472] Dynamo bytecode transform time: 10.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|██████████| 6/6 [00:01<00:00,  4.19it/s, triton_poi_fused_add_mul_sub_5]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 03:20:25 [backends.py:161] Cache the graph of shape None for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 12.18it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 79.15it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 58.77it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 148.22it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 137.33it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 148.72it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 114.54it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 167.46it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 172.54it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 155.86it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 95.62it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 155.83it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 174.24it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 116.59it/s, triton_poi_fused_add_mul_sub_9]                  \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 96.85it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 165.18it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 164.13it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 87.64it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 154.02it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 147.47it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 82.70it/s, triton_poi_fused_add_mul_sub_9]                    \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 138.91it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 161.50it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 88.14it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 148.74it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 162.74it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 111.32it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 156.85it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 160.98it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 154.99it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 76.97it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 150.88it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 126.48it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 141.74it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 10/10 [00:00<00:00, 147.22it/s, triton_poi_fused_add_mul_sub_9]                   \n",
      "Inductor Compilation: 100%|██████████| 5/5 [00:00<00:00, 26.16it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 03:21:01 [backends.py:173] Compiling a graph for general shape takes 39.12 s\n",
      "INFO 06-17 03:22:38 [monitor.py:34] torch.compile takes 49.70 s in total\n",
      "INFO 06-17 03:22:41 [gpu_worker.py:227] Available KV cache memory: 11.30 GiB\n",
      "INFO 06-17 03:22:42 [kv_cache_utils.py:715] GPU KV cache size: 82,256 tokens\n",
      "INFO 06-17 03:22:42 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 20.08x\n",
      "INFO 06-17 03:24:25 [gpu_model_runner.py:2048] Graph capturing finished in 104 secs, took 1.84 GiB\n",
      "INFO 06-17 03:24:25 [core.py:171] init engine (profile, create kv cache, warmup model) took 255.20 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: The tokenizer `/data/download-model/DeepSeek-R1-0528-Qwen3-8B`\ndoes not have a {% if add_generation_prompt %} for generation purposes.\nPlease file a bug report to the maintainers of `/data/download-model/DeepSeek-R1-0528-Qwen3-8B` - thanks!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m lora_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;66;03m# Larger rank = smarter, but slower\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/download-model/DeepSeek-R1-0528-Qwen3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# False for LoRA 16bit\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Enable vLLM fast inference\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduce if out of memory\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mget_peft_model(\n\u001b[1;32m     16\u001b[0m     model,\n\u001b[1;32m     17\u001b[0m     r \u001b[38;5;241m=\u001b[39m lora_rank, \u001b[38;5;66;03m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3407\u001b[39m,\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/loader.py:376\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/qwen3.py:419\u001b[0m, in \u001b[0;36mFastQwen3Model.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(  \u001b[38;5;66;03m#TODO: Change after release\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    418\u001b[0m ):\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFastQwen3Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:1893\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;66;03m# Counteract saved tokenizers\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m tokenizer_name \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer_name\n\u001b[0;32m-> 1893\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_correct_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1902\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m patch_tokenizer(model, tokenizer)\n\u001b[1;32m   1903\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m model_patcher\u001b[38;5;241m.\u001b[39mpost_patch(model, tokenizer)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/tokenizer_utils.py:586\u001b[0m, in \u001b[0;36mload_correct_tokenizer\u001b[0;34m(tokenizer_name, model_max_length, padding_side, token, trust_remote_code, cache_dir, fix_tokenizer)\u001b[0m\n\u001b[1;32m    583\u001b[0m     chat_template \u001b[38;5;241m=\u001b[39m old_chat_template\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 586\u001b[0m     chat_template \u001b[38;5;241m=\u001b[39m \u001b[43mfix_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_chat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m chat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Fixing chat template failed - please file a report immediately!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m         )\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/tokenizer_utils.py:686\u001b[0m, in \u001b[0;36mfix_chat_template\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m    683\u001b[0m new_chat_template \u001b[38;5;241m=\u001b[39m _fix_chat_template(chat_template)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% i\u001b[39;00m\u001b[38;5;124mf add_generation_prompt \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m new_chat_template \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%- i\u001b[39;00m\u001b[38;5;124mf add_generation_prompt \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m new_chat_template:\n\u001b[0;32m--> 686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: The tokenizer `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mname_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not have a \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% i\u001b[39;00m\u001b[38;5;124mf add_generation_prompt \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m} for generation purposes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease file a bug report to the maintainers of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mname_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` - thanks!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m     )\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: We successfully patched the tokenizer to add a \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% i\u001b[39;00m\u001b[38;5;124mf add_generation_prompt \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m} to the chat_template.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is not a bug, but please notify the maintainers of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mname_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` - thanks!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    695\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: The tokenizer `/data/download-model/DeepSeek-R1-0528-Qwen3-8B`\ndoes not have a {% if add_generation_prompt %} for generation purposes.\nPlease file a bug report to the maintainers of `/data/download-model/DeepSeek-R1-0528-Qwen3-8B` - thanks!"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "model_name=\"/data/download-model/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76e3b7-52a9-4ebe-ab51-c8aedb1e95e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecb425-ea12-4170-96fa-d77548d98739",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = None\n",
    "reasoning_end = None\n",
    "user_token = None\n",
    "assistant_token = None\n",
    "\n",
    "for token in tokenizer.get_added_vocab().keys():\n",
    "    if \"think\" in token and \"/\" in token:\n",
    "        reasoning_end = token\n",
    "    elif \"think\" in token:\n",
    "        reasoning_start = token\n",
    "    elif \"user\" in token:\n",
    "        user_token = token\n",
    "    elif \"assistant\" in token:\n",
    "        assistant_token = token\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "You must think in Bahasa Indonesia.\"\"\"\n",
    "system_prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
