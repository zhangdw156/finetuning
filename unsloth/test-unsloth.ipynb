{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fdc6bc-ca34-4eae-a310-0a00a02d1b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b2f7a-2ee9-4cb9-9ac3-35024b228579",
   "metadata": {},
   "source": [
    "## æµ‹è¯•æ¡†æ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2246d9-dd7e-41de-a067-8634561c437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 20480 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa1a492-9e25-4599-adda-452c7aff47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/data/download-model/Qwen3-14B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b8a4b8-5f57-4557-9b14-80b95d3fc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_4bit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9ecd21-8c37-402f-bcd3-c321b43f85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f24cfb2-bc8b-4dc2-afe1-25bcbc1e25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H20. Num GPUs = 8. Max memory: 94.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4698feca8042c3a7ec51743df54100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(model_name = model_path,max_seq_length = max_seq_length,dtype = dtype,load_in_4bit = load_in_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099468f2-409e-4782-99a7-0555e2d2f658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "          (down_proj): Linear(in_features=17408, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82486af8-b18e-4d8f-80ce-0c52c3cf380a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/data/download-model/Qwen3-14B', vocab_size=151643, model_max_length=40960, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a70232-e7ef-4a5c-911a-1bd11e3532e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 5120, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=17408, bias=False)\n",
       "          (down_proj): Linear(in_features=17408, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694b3ec7-a3ae-4431-9dbb-56a3d901c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ä½ æ˜¯è°ï¼Ÿ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42493069-fdee-495c-aa2e-75cb0200e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e148f60-fb10-49ad-a5ec-2c6ce13dd9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[105043, 100165,  11319]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a89e41-42f9-4243-9b6f-a37b12499021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[105043, 100165,      0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"ä½ æ˜¯è°!\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d403ede3-e3e3-42f6-a1b1-8521a931ee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[105043]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"ä½ æ˜¯\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6b6fa7-02cc-466f-8b7b-eaccdf1d463e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[100165]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"è°\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a0c1df-910c-47bf-958c-a493bb1927d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[0]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"!\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60bb3a76-bb24-4806-b8bf-135acf4ca609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30]], device='cuda:0'), 'attention_mask': tensor([[1]], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"?\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0141964-445e-47af-8d0c-d56842ad9cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[100165,     30]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"è°?\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1882794a-69f2-4cb4-906e-b2714546d174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[105043, 100165,  11319]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"ä½ æ˜¯è°ï¼Ÿ\"\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea91f19e-f9cc-4f82-b7f2-d5a7ebc5e5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[105043, 100165,  11319,  56568, 104139,  98380,  26850, 106287,   3837,\n",
       "          20002, 107557,   2073, 105043, 100165,  11319,  56568, 104139,  98380,\n",
       "          11319,  33590,  35946,  85106, 100700, 102104,   1773, 101140,   3837,\n",
       "          35946,  99730, 100157, 100005, 101294,   3837, 102200,  31935,  64559,\n",
       "          99320,  56007,   3837,  20412, 107076, 100338, 111477,  31935,  64559,\n",
       "         104800, 100048,   9370,  71304, 105483, 102064, 104949,   1773, 101889,\n",
       "           3837, 104515, 118569,  97611,  99558,  98380,   3837, 101912, 102104,\n",
       "          86119,   5373, 104223,  87335,   5373, 104913, 113272,   5373, 110569,\n",
       "          49567,   1773,  91572,   3837,  35946, 104019, 104496,  97611,  42140,\n",
       "         102064, 100143,  99788,   3837, 100630, 104811,   5373, 105205,   5373,\n",
       "          24339,  72881,   5373, 105576,  72881,  49567,     16,     15,     15,\n",
       "         101312, 102064,   1773, 101948,   3837,  35946,  85106, 104046,  35946,\n",
       "         100006, 100345, 107494, 109504,  43959,  99604, 104040,  33108,  98402,\n",
       "           9370, 108704,   3837, 101912,  61443, 101108,   5373,  61443, 102955,\n",
       "           5373,  61443, 109620,  49567,   1773, 104019,  66394, 109944,  71817,\n",
       "          42140,  99620, 105051,   3837, 101128, 102285,  16744,   3837,  99553,\n",
       "          54926, 100116,   9370, 104787,   1773, 104043,   3837,  35946,  85106,\n",
       "         104496,  35946, 100006,  54542,  33108, 101042,  20074,   3837,  43959,\n",
       "         116996,  33108, 100370,   3837, 100364,  20002,  71817, 102041,   1773,\n",
       "         100161,   3837,  35946,  99730, 104211,  20002,   3837, 103925,  35946,\n",
       "         110121,  99553, 102188, 105427,   3837,  77288, 104309,  18830,  32100,\n",
       "           3837, 101898,  20002, 107002,  99335,  27369,   1773, 101908, 102104,\n",
       "          30534, 100662, 113113,  32108,   3837, 101153,  37029,  99878, 116925,\n",
       "           3837, 115987, 100047, 101128,   1773,  91572,   3837,  30534, 103944,\n",
       "          27369, 102188,   3837,  16530, 114078, 114247,   3837, 100166, 104542,\n",
       "           3837,  17177,  27442,  66394,   1773, 104019,  60533, 100148,  37029,\n",
       "          68005,  68805,   3837, 100662,  99795,   9370, 113113, 102124,   1773,\n",
       "          99601, 113841,  43815,  99877,  12857,  46944,  54926, 100116, 111423,\n",
       "           3837, 103944, 100795,  55338, 108164,  90395, 100136, 110205,  99795,\n",
       "           8997,  99692,   3837,  35946,  36407, 100700, 102104, 103929,  86119,\n",
       "           3407, 104198,  31935,  64559,  99320,  56007,   3837,  20412, 107076,\n",
       "         100338, 111477,  31935,  64559, 104800, 100048,   9370,  71304, 105483,\n",
       "         102064, 104949,   1773,  97611,  99558,  98380, 100630,  48443,     16,\n",
       "             13,   3070, 102104,  86119,    334,   5122, 109944, 102104, 100646,\n",
       "         100650, 103936,   3837, 100630,  99891,   5373,  99361,   5373,  99348,\n",
       "           5373,  99424,  49567,   3837, 100364,  56568, 101098,  45912,  27369,\n",
       "           3407,     17,     13,   3070, 104223,  87335,    334,   5122, 109944,\n",
       "         100345, 103929, 100354, 104223,  99604, 104040,  33108,  98402,   9370,\n",
       "         108704,   3837, 101912,  61443, 101108,   5373,  61443, 102955,   5373,\n",
       "          61443, 109620,   5373,  61443, 107604,  49567,   3407,     18,     13,\n",
       "           3070, 104913, 113272,    334,   5122,  35946, 100006,  71817, 104913,\n",
       "         113272,   3837, 100638, 104552,  86119,   5373, 101042, 102181,  99559,\n",
       "          90395,  99553, 105630, 104520,   3407,     19,     13,   3070, 110569,\n",
       "          99788,    334,   5122, 109944, 108598,  33108, 110760,  46100,   3837,\n",
       "         100143, 101312, 110569, 102064,   3837,  29524,  30280,   5373,  15041,\n",
       "           5373,     34,   1027,  49567,   3837, 100364,  56568, 100638, 110569,\n",
       "          86119,   3407,     20,     13,   3070,  42140, 102064, 100143,    334,\n",
       "           5122,  35946, 100143,     16,     15,     15, 101312, 102064,   3837,\n",
       "         100630, 104811,   5373, 105205,   5373,  24339,  72881,   5373, 105576,\n",
       "          72881,   5373,  99462,  72881,  49567,   3837,  73670,  71817,  99960,\n",
       "         102064, 101069,  33108, 105395,   3407,     21,     13,   3070,  42140,\n",
       "          99620, 105051,    334,   5122, 109944,  71817,  42140,  99620, 105051,\n",
       "           3837, 101128, 102285,  16744,   3837,  99553,  54926, 100116,  33108,\n",
       "          99795,   9370, 104787,   3837, 100231, 102612, 101069,   3407,     22,\n",
       "             13,   3070, 111540,  57218,  54542,    334,   5122, 109944,  54542,\n",
       "          33108, 101042,  20074,   3837,  43959, 116996,   5373, 100370,   3837,\n",
       "         100364,  56568,  71817, 102041, 101042,   3407,     23,     13,   3070,\n",
       "         106412,  47874,    334,   5122, 100345, 103929, 100354,   3837, 109944,\n",
       "         101921, 102104,   9370, 104040,  33108,  43815,   3837,  99553,  33126,\n",
       "         102647, 105302,  47874,   3407, 107916, 100146,   3837, 103925,  35946,\n",
       "         110121,  99553, 102188, 105427,   3837,  77288, 104309,  18830,  32100,\n",
       "          57191,  16530, 100677, 103958,   3837, 101898,  18493, 102031,  99335,\n",
       "         102041,  13343,  71817, 107002,   1773, 102056, 110117, 100398,  86119,\n",
       "          57191,  85106, 100364,   3837, 102422, 106525,   6313,  26525,    232,\n",
       "            271,  44364,  14374,  90476,    119,  36885,    271,     12,   3070,\n",
       "         101294,    334,   5122,  31935,  64559,  99320,  56007,   3837,  71304,\n",
       "         105483, 102064, 104949,   8997,     12,   3070,  98380,    334,   5122,\n",
       "         102104,  86119,   5373, 104223,  87335,   5373, 104913, 113272,   5373,\n",
       "         110569,   5373,  42140, 102064, 100143,   5373,  42140,  99620, 105051,\n",
       "           5373, 111540,   8997,     12,   3070, 100772,    334,   5122, 100143,\n",
       "             16,     15,     15, 101312, 102064,   3837,  99553, 106412,  47874,\n",
       "           3837, 100231, 101312, 116541,   3407,  99880, 100001,  27369, 105988,\n",
       "          18830, 100364,   6313, 107055,  92894,  86119,   3837, 100437, 100640,\n",
       "         107666,   1773,  26525,    232, 151645]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=inputs.input_ids,attention_mask=inputs.attention_mask,max_new_tokens=1200,use_cache=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdd98025-3c44-455a-92f0-21a23eaacb03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ä½ æ˜¯è°ï¼Ÿä½ æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ\\n\\nå—¯ï¼Œç”¨æˆ·é—®æˆ‘â€œä½ æ˜¯è°ï¼Ÿä½ æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿâ€ï¼Œæˆ‘éœ€è¦è¯¦ç»†å›ç­”ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥ä»‹ç»è‡ªå·±çš„èº«ä»½ï¼Œä¹Ÿå°±æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘è¦åˆ—ä¸¾æˆ‘çš„ä¸»è¦åŠŸèƒ½ï¼Œæ¯”å¦‚å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ã€‚åŒæ—¶ï¼Œæˆ‘è¿˜è¦æåˆ°æˆ‘çš„å¤šè¯­è¨€æ”¯æŒèƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ç­‰100å¤šç§è¯­è¨€ã€‚å¦å¤–ï¼Œæˆ‘éœ€è¦å¼ºè°ƒæˆ‘èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æŒ‡ä»¤ç”Ÿæˆä¸åŒé£æ ¼å’Œé•¿åº¦çš„æ–‡æœ¬ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ã€‚è¿˜è¦è¯´æ˜æˆ‘å¯ä»¥è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡ï¼Œæä¾›è¿è´¯çš„å›å¤ã€‚æ­¤å¤–ï¼Œæˆ‘éœ€è¦æåˆ°æˆ‘èƒ½å¤Ÿå¤„ç†å’Œåˆ†ææ•°æ®ï¼Œç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Šï¼Œå¸®åŠ©ç”¨æˆ·è¿›è¡Œå†³ç­–ã€‚æœ€åï¼Œæˆ‘åº”è¯¥æé†’ç”¨æˆ·ï¼Œè™½ç„¶æˆ‘å°½åŠ›æä¾›å‡†ç¡®çš„ä¿¡æ¯ï¼Œä½†å¯èƒ½ä¼šæœ‰é”™è¯¯ï¼Œå»ºè®®ç”¨æˆ·æ ¸å®é‡è¦ä¿¡æ¯ã€‚æ•´ä¸ªå›ç­”è¦ä¿æŒå£è¯­åŒ–ï¼Œé¿å…ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œè®©ç”¨æˆ·å®¹æ˜“ç†è§£ã€‚åŒæ—¶ï¼Œè¦ç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼Œä¸é—æ¼é‡è¦å†…å®¹ï¼Œç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹è¯´æ˜ã€‚è¿˜è¦æ³¨æ„ä¸è¦ä½¿ç”¨Markdownæ ¼å¼ï¼Œä¿æŒè‡ªç„¶çš„å£è¯­è¡¨è¾¾ã€‚ç°åœ¨æŠŠè¿™äº›å†…å®¹ç»„ç»‡æˆä¸€ä¸ªè¿è´¯çš„å›ç­”ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰è¦ç‚¹ï¼Œå¹¶ä¸”æµç•…è‡ªç„¶ã€‚\\nå¥½çš„ï¼Œæˆ‘æ¥è¯¦ç»†å›ç­”ä½ çš„é—®é¢˜ã€‚\\n\\næˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\\n\\n1. **å›ç­”é—®é¢˜**ï¼šæˆ‘å¯ä»¥å›ç­”å„ç§é¢†åŸŸçš„é—®é¢˜ï¼ŒåŒ…æ‹¬ç§‘å­¦ã€æŠ€æœ¯ã€æ–‡åŒ–ã€ç”Ÿæ´»ç­‰ï¼Œå¸®åŠ©ä½ å¿«é€Ÿè·å–ä¿¡æ¯ã€‚\\n\\n2. **åˆ›ä½œæ–‡å­—**ï¼šæˆ‘å¯ä»¥æ ¹æ®ä½ çš„éœ€æ±‚åˆ›ä½œä¸åŒé£æ ¼å’Œé•¿åº¦çš„æ–‡æœ¬ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€å†™è¯—æ­Œç­‰ã€‚\\n\\n3. **é€»è¾‘æ¨ç†**ï¼šæˆ‘èƒ½å¤Ÿè¿›è¡Œé€»è¾‘æ¨ç†ï¼Œè§£å†³æ•°å­¦é—®é¢˜ã€åˆ†æå¤æ‚æƒ…å†µï¼Œå¹¶æä¾›åˆç†çš„è§£å†³æ–¹æ¡ˆã€‚\\n\\n4. **ç¼–ç¨‹èƒ½åŠ›**ï¼šæˆ‘å¯ä»¥ç¼–å†™å’Œè°ƒè¯•ä»£ç ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œå¦‚Pythonã€Javaã€C++ç­‰ï¼Œå¸®åŠ©ä½ è§£å†³ç¼–ç¨‹é—®é¢˜ã€‚\\n\\n5. **å¤šè¯­è¨€æ”¯æŒ**ï¼šæˆ‘æ”¯æŒ100å¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ç­‰ï¼Œå¯ä»¥è¿›è¡Œè·¨è¯­è¨€äº¤æµå’Œç¿»è¯‘ã€‚\\n\\n6. **å¤šè½®å¯¹è¯**ï¼šæˆ‘å¯ä»¥è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡ï¼Œæä¾›è¿è´¯å’Œè‡ªç„¶çš„å›å¤ï¼Œé€‚åˆé•¿æ—¶é—´äº¤æµã€‚\\n\\n7. **æ•°æ®åˆ†æä¸å¤„ç†**ï¼šæˆ‘å¯ä»¥å¤„ç†å’Œåˆ†ææ•°æ®ï¼Œç”Ÿæˆå›¾è¡¨ã€æŠ¥å‘Šï¼Œå¸®åŠ©ä½ è¿›è¡Œå†³ç­–åˆ†æã€‚\\n\\n8. **ä¸ªæ€§åŒ–æœåŠ¡**ï¼šæ ¹æ®ä½ çš„éœ€æ±‚ï¼Œæˆ‘å¯ä»¥è°ƒæ•´å›ç­”çš„é£æ ¼å’Œå†…å®¹ï¼Œæä¾›æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚\\n\\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶æˆ‘å°½åŠ›æä¾›å‡†ç¡®çš„ä¿¡æ¯ï¼Œä½†å¯èƒ½ä¼šæœ‰é”™è¯¯æˆ–ä¸å®Œå–„çš„åœ°æ–¹ï¼Œå»ºè®®åœ¨æ¶‰åŠé‡è¦å†³ç­–æ—¶è¿›è¡Œæ ¸å®ã€‚å¦‚æœä½ æœ‰ä»»ä½•å…·ä½“é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ ğŸ˜Š\\n\\n---\\n\\n### æ€»ç»“\\n\\n- **èº«ä»½**ï¼šé€šä¹‰åƒé—®ï¼Œè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚\\n- **åŠŸèƒ½**ï¼šå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ã€å¤šè¯­è¨€æ”¯æŒã€å¤šè½®å¯¹è¯ã€æ•°æ®åˆ†æã€‚\\n- **ç‰¹ç‚¹**ï¼šæ”¯æŒ100å¤šç§è¯­è¨€ï¼Œæä¾›ä¸ªæ€§åŒ–æœåŠ¡ï¼Œé€‚åˆå¤šç§åº”ç”¨åœºæ™¯ã€‚\\n\\nå¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœæœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­æé—®ã€‚ ğŸ˜Š<|im_end|>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(outputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391be2e-5c01-44cf-a092-3af6f10f5da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d559465-7f66-40b1-b36c-05e534e7b8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n### Question:\\n{}\\n### Response:\\n<think>{}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "### Question:\n",
    "{}\n",
    "### Response:\n",
    "<think>{}\"\"\"\n",
    "prompt_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b8c0d1a-cc52-4727-a7a7-8680bd6d0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”å¦‚ä½•å¤„ç†ï¼Ÿ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1319d7cd-a21a-4b40-9e6f-3986530601fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 38214,    374,    458,   7600,    429,  16555,    264,   3383,     11,\n",
       "          34426,    448,    458,   1946,    429,   5707,   4623,   2266,     13,\n",
       "            715,   7985,    264,   2033,    429,  34901,  44595,    279,   1681,\n",
       "             13,    715,  10227,  35764,     11,   1744,  15516,    911,    279,\n",
       "           3405,    323,   1855,    264,   3019,  14319,  29208,   8781,    315,\n",
       "          11303,    311,   5978,    264,  19819,    323,  13382,   2033,    624,\n",
       "          14374,  29051,    510,   2610,    525,    264,   6457,   6203,    448,\n",
       "          10847,   6540,    304,  14490,  32711,     11,  49418,     11,    323,\n",
       "           6380,   9115,     13,    715,   5501,   4226,    279,   2701,   6457,\n",
       "           3405,     13,    715,  14374,  15846,    510,  46944, 110088, 109689,\n",
       "         121033, 101143, 100439,   9370, 104693,  99461, 103202,     20,  35727,\n",
       "           3837, 101317, 100406,  93266,  18830, 106104,  77288, 104187, 108976,\n",
       "          96050, 106481,  13343,  99879,  64817,  16872, 101317,  18830,  99451,\n",
       "         100406,   9370,  67279,  99922,   3837, 104276,  50511, 100007,  54542,\n",
       "          94432,  14374,   5949,    510, 151667]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89c779bd-7968-4887-952c-d0efb2214bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 38214,    374,    458,  ..., 104160,   1773, 151645]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18102041-9988-4c07-bbf1-97a09399d78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n### Question:\\nä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”å¦‚ä½•å¤„ç†ï¼Ÿ\\n### Response:\\n<think>\\nå—¯ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å…³äºæ€¥æ€§é˜‘å°¾ç‚çš„å¤„ç†ï¼Œç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›å‡è½»ä½†è¿˜æœ‰å‘çƒ­ï¼Œä½“æ£€å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦å›å¿†æ€¥æ€§é˜‘å°¾ç‚çš„ç—…ç¨‹å‘å±•ã€‚é€šå¸¸æ€¥æ€§é˜‘å°¾ç‚åˆæœŸä¼šæœ‰è½¬ç§»æ€§å³ä¸‹è…¹ç—›ï¼Œç„¶åé€æ¸åŠ é‡ï¼Œä½†å‘ç—…5å¤©åï¼Œå¦‚æœè…¹ç—›å‡è½»ï¼Œå¯èƒ½æ„å‘³ç€ç—…æƒ…æœ‰æ‰€å˜åŒ–ã€‚\\n\\nè¿™æ—¶å€™è¦è€ƒè™‘å¹¶å‘ç—‡çš„å¯èƒ½ï¼Œæ¯”å¦‚é˜‘å°¾ç©¿å­”æˆ–è€…å½¢æˆé˜‘å°¾å‘¨å›´è„“è‚¿ã€‚å› ä¸ºå¦‚æœé˜‘å°¾ç©¿å­”ï¼Œå¯èƒ½ä¼šå¯¼è‡´å±€éƒ¨ç‚ç—‡æ‰©æ•£ï¼Œå½¢æˆè„“è‚¿ï¼Œè¿™æ—¶å€™è…¹ç—›å¯èƒ½å‡è½»ï¼Œä½†å‘çƒ­æŒç»­ï¼Œç”šè‡³å‡ºç°åŒ…å—ã€‚ä½“æ£€å‘ç°çš„å‹ç—›åŒ…å—å¯èƒ½æ˜¯è„“è‚¿å½¢æˆçš„è¿¹è±¡ã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘å¤„ç†åŸåˆ™ã€‚å¯¹äºæ€¥æ€§é˜‘å°¾ç‚ï¼Œé€šå¸¸çš„æ²»ç–—æ˜¯æ‰‹æœ¯åˆ‡é™¤é˜‘å°¾ï¼Œä¹Ÿå°±æ˜¯é˜‘å°¾åˆ‡é™¤æœ¯ã€‚ä½†å¦‚æœæ˜¯å·²ç»å½¢æˆè„“è‚¿çš„æƒ…å†µï¼Œå¯èƒ½éœ€è¦å…ˆè¿›è¡ŒæŠ—ç”Ÿç´ æ²»ç–—ï¼Œæ§åˆ¶æ„ŸæŸ“ï¼Œå¾…è„“è‚¿å±€é™åŒ–åï¼Œå†è€ƒè™‘æ‰‹æœ¯ã€‚æˆ–è€…ï¼Œå¦‚æœè„“è‚¿è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦ç©¿åˆºå¼•æµã€‚\\n\\nä¸è¿‡ï¼Œè¿™é‡Œçš„æƒ…å†µæ˜¯å‘ç—…5å¤©ï¼Œè…¹ç—›å‡è½»ä½†ä»æœ‰å‘çƒ­ï¼Œå¯èƒ½å¤„äºé˜‘å°¾å‘¨å›´è„“è‚¿çš„é˜¶æ®µã€‚è¿™æ—¶å€™å¦‚æœæ‚£è€…ä¸€èˆ¬æƒ…å†µç¨³å®šï¼Œå¯ä»¥å…ˆç”¨æŠ—ç”Ÿç´ æ²»ç–—ï¼ŒåŒæ—¶è§‚å¯Ÿæ˜¯å¦æœ‰è„“è‚¿å½¢æˆã€‚å¦‚æœåŒ…å—æŒç»­å­˜åœ¨æˆ–è€…æ„ŸæŸ“æ§åˆ¶ä¸ä½³ï¼Œå¯èƒ½éœ€è¦æ‰‹æœ¯å¼•æµæˆ–è€…åˆ‡é™¤ã€‚\\n\\nå¦å¤–ï¼Œè¿˜è¦è€ƒè™‘æ‚£è€…çš„æ•´ä½“çŠ¶å†µï¼Œæ¯”å¦‚æ˜¯å¦æœ‰å…¶ä»–å¹¶å‘ç—‡ï¼Œå¦‚è…¹è†œç‚çš„è¿¹è±¡ï¼Œæˆ–è€…æ˜¯å¦å­˜åœ¨å…¶ä»–æ„ŸæŸ“æºã€‚åŒæ—¶ï¼Œå½±åƒå­¦æ£€æŸ¥å¦‚è¶…å£°æˆ–CTå¯èƒ½æœ‰åŠ©äºæ˜ç¡®è¯Šæ–­ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰è„“è‚¿å½¢æˆæˆ–é˜‘å°¾ç©¿å­”ã€‚\\n\\næ‰€ä»¥å¤„ç†æ­¥éª¤å¯èƒ½åŒ…æ‹¬ï¼š1. è¿›è¡Œå½±åƒå­¦æ£€æŸ¥ç¡®è®¤æ˜¯å¦æœ‰è„“è‚¿ï¼›2. å¼€å§‹å¹¿è°±æŠ—ç”Ÿç´ æ²»ç–—ï¼›3. æ ¹æ®æ£€æŸ¥ç»“æœå†³å®šæ˜¯å¦éœ€è¦æ‰‹æœ¯å¼•æµæˆ–åˆ‡é™¤ã€‚å¦‚æœæ‚£è€…æƒ…å†µç¨³å®šï¼Œå¯ä»¥å…ˆä¿å®ˆæ²»ç–—ï¼Œå¦åˆ™éœ€è¦æ‰‹æœ¯å¹²é¢„ã€‚\\n</think>\\n\\nå¯¹äºè¯¥æ‚£è€…ï¼Œå¤„ç†éœ€éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š\\n\\n**1. æ˜ç¡®è¯Šæ–­ä¸è¯„ä¼°å¹¶å‘ç—‡**\\n- **ç—…ç¨‹åˆ†æ**ï¼šæ€¥æ€§é˜‘å°¾ç‚å‘ç—…5å¤©åè…¹ç—›å‡è½»ä½†æŒç»­å‘çƒ­ï¼Œæç¤ºå¯èƒ½è¿›å…¥**é˜‘å°¾å‘¨å›´è„“è‚¿å½¢æˆæœŸ**ï¼ˆå³é˜‘å°¾ç©¿å­”åå±€éƒ¨æ„ŸæŸ“å±€é™åŒ–é˜¶æ®µï¼‰ã€‚å³ä¸‹è…¹å‹ç—›æ€§åŒ…å—è¿›ä¸€æ­¥æ”¯æŒæ­¤è¯Šæ–­ã€‚\\n- **é‰´åˆ«è¯Šæ–­**ï¼šéœ€æ’é™¤å…¶ä»–å¯èƒ½ï¼ˆå¦‚è‚ ç»“æ ¸ã€å¦‡ç§‘ç–¾ç—…ç­‰ï¼‰ï¼Œä½†ç»“åˆå…¸å‹ç—…å²å’Œä½“å¾ï¼Œè„“è‚¿å¯èƒ½æ€§æœ€å¤§ã€‚\\n\\n**2. å½±åƒå­¦æ£€æŸ¥**\\n- **é¦–é€‰è¶…å£°æˆ–CT**ï¼šç¡®è®¤å³ä¸‹è…¹æ˜¯å¦å­˜åœ¨**é˜‘å°¾å‘¨å›´è„“è‚¿**ï¼ˆå¦‚è„“æ¶²ç§¯èšã€å‘¨å›´è„‚è‚ªé—´éš™æ¨¡ç³Šç­‰ï¼‰ï¼Œæ˜ç¡®è„“è‚¿èŒƒå›´åŠæ˜¯å¦ç´¯åŠé‚»è¿‘å™¨å®˜ï¼ˆå¦‚è¾“å°¿ç®¡ã€è†€èƒ±ï¼‰ã€‚\\n\\n**3. æ²»ç–—ç­–ç•¥**\\n- **åˆå§‹æ²»ç–—ï¼šæŠ—ç”Ÿç´ æ§åˆ¶æ„ŸæŸ“**\\n  - é€‰æ‹©**å¹¿è°±æŠ—ç”Ÿç´ **ï¼ˆå¦‚å¤´å­¢ç±»è”åˆç”²ç¡å”‘ï¼‰ï¼Œè¦†ç›–é©å…°æ°é˜´æ€§èŒå’ŒåŒæ°§èŒã€‚\\n  - è‹¥æ‚£è€…å­˜åœ¨è„“æ¯’ç—‡å¾è±¡ï¼ˆå¦‚é«˜çƒ­ã€ç™½ç»†èƒæ˜¾è‘—å‡é«˜ï¼‰ï¼Œéœ€é™è„‰ç»™è¯ã€‚\\n- **è„“è‚¿å¤„ç†**\\n  - **ä¿å®ˆæ²»ç–—**ï¼šè‹¥è„“è‚¿è¾ƒå°ï¼ˆ<5cmï¼‰ã€æ‚£è€…ä¸€èˆ¬çŠ¶å†µè‰¯å¥½ï¼Œå¯ç»§ç»­æŠ—ç”Ÿç´ æ²»ç–—ï¼Œè§‚å¯Ÿè„“è‚¿æ˜¯å¦è‡ªè¡Œå¸æ”¶ã€‚\\n  - **æ‰‹æœ¯å¹²é¢„**ï¼šè‹¥è„“è‚¿è¾ƒå¤§ï¼ˆ>5cmï¼‰ã€æŒç»­å‘çƒ­æˆ–æ„ŸæŸ“æ§åˆ¶ä¸ä½³ï¼Œéœ€è¡Œ**ç»çš®ç©¿åˆºå¼•æµ**ï¼ˆè¶…å£°å¼•å¯¼ä¸‹ï¼‰æˆ–**è…¹è…”é•œå¼•æµ**ã€‚è‹¥æ‚£è€…ç—…æƒ…ç¨³å®šï¼Œå¯åœ¨æ„ŸæŸ“æ§åˆ¶å2-4å‘¨è¡Œ**é˜‘å°¾åˆ‡é™¤æœ¯**ï¼ˆé¿å…å¤å‘ï¼‰ã€‚\\n\\n**4. ç›‘æµ‹ä¸éšè®¿**\\n- å¯†åˆ‡è§‚å¯Ÿä½“æ¸©ã€ç™½ç»†èƒè®¡æ•°åŠå±€éƒ¨ä½“å¾å˜åŒ–ã€‚\\n- è‹¥è„“è‚¿æœªæ¶ˆé€€æˆ–å‡ºç°å¹¶å‘ç—‡ï¼ˆå¦‚è„“æ¯’ç—‡ã€è‚ æ¢—é˜»ï¼‰ï¼Œéœ€åŠæ—¶æ‰‹æœ¯ã€‚\\n\\n**5. ç‰¹æ®Šæ³¨æ„äº‹é¡¹**\\n- **é¿å…è¿‡æ—©æ‰‹æœ¯**ï¼šè‹¥æœªæ˜ç¡®è„“è‚¿å½¢æˆï¼Œè¿‡æ—©æ‰‹æœ¯å¯èƒ½å¢åŠ æ„ŸæŸ“æ‰©æ•£é£é™©ã€‚\\n- **å¦Šå¨ æ‚£è€…**ï¼šè‹¥ä¸ºå¥³æ€§éœ€æ’é™¤å¼‚ä½å¦Šå¨ ï¼Œè¶…å£°æ£€æŸ¥éœ€æ³¨æ„å­å®«åŠé™„ä»¶æƒ…å†µã€‚\\n\\n**æ€»ç»“**ï¼šå½“å‰åº”å…ˆè¿›è¡Œå½±åƒå­¦æ£€æŸ¥æ˜ç¡®è„“è‚¿å­˜åœ¨ï¼Œéšåä»¥æŠ—ç”Ÿç´ æ²»ç–—ä¸ºä¸»ï¼Œæ ¹æ®è„“è‚¿å¤§å°åŠæ„ŸæŸ“æ§åˆ¶æƒ…å†µå†³å®šæ˜¯å¦éœ€å¼•æµæˆ–å»¶æœŸæ‰‹æœ¯ã€‚<|im_end|>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(outputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea628d34-15c7-4878-a3d9-71ce43f2576e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯å…³äºæ€¥æ€§é˜‘å°¾ç‚çš„å¤„ç†ï¼Œç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›å‡è½»ä½†è¿˜æœ‰å‘çƒ­ï¼Œä½“æ£€å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦å›å¿†æ€¥æ€§é˜‘å°¾ç‚çš„ç—…ç¨‹å‘å±•ã€‚é€šå¸¸æ€¥æ€§é˜‘å°¾ç‚åˆæœŸä¼šæœ‰è½¬ç§»æ€§å³ä¸‹è…¹ç—›ï¼Œç„¶åé€æ¸åŠ é‡ï¼Œä½†å‘ç—…5å¤©åï¼Œå¦‚æœè…¹ç—›å‡è½»ï¼Œå¯èƒ½æ„å‘³ç€ç—…æƒ…æœ‰æ‰€å˜åŒ–ã€‚\n",
      "\n",
      "è¿™æ—¶å€™è¦è€ƒè™‘å¹¶å‘ç—‡çš„å¯èƒ½ï¼Œæ¯”å¦‚é˜‘å°¾ç©¿å­”æˆ–è€…å½¢æˆé˜‘å°¾å‘¨å›´è„“è‚¿ã€‚å› ä¸ºå¦‚æœé˜‘å°¾ç©¿å­”ï¼Œå¯èƒ½ä¼šå¯¼è‡´å±€éƒ¨ç‚ç—‡æ‰©æ•£ï¼Œå½¢æˆè„“è‚¿ï¼Œè¿™æ—¶å€™è…¹ç—›å¯èƒ½å‡è½»ï¼Œä½†å‘çƒ­æŒç»­ï¼Œç”šè‡³å‡ºç°åŒ…å—ã€‚ä½“æ£€å‘ç°çš„å‹ç—›åŒ…å—å¯èƒ½æ˜¯è„“è‚¿å½¢æˆçš„è¿¹è±¡ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘å¤„ç†åŸåˆ™ã€‚å¯¹äºæ€¥æ€§é˜‘å°¾ç‚ï¼Œé€šå¸¸çš„æ²»ç–—æ˜¯æ‰‹æœ¯åˆ‡é™¤é˜‘å°¾ï¼Œä¹Ÿå°±æ˜¯é˜‘å°¾åˆ‡é™¤æœ¯ã€‚ä½†å¦‚æœæ˜¯å·²ç»å½¢æˆè„“è‚¿çš„æƒ…å†µï¼Œå¯èƒ½éœ€è¦å…ˆè¿›è¡ŒæŠ—ç”Ÿç´ æ²»ç–—ï¼Œæ§åˆ¶æ„ŸæŸ“ï¼Œå¾…è„“è‚¿å±€é™åŒ–åï¼Œå†è€ƒè™‘æ‰‹æœ¯ã€‚æˆ–è€…ï¼Œå¦‚æœè„“è‚¿è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦ç©¿åˆºå¼•æµã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œè¿™é‡Œçš„æƒ…å†µæ˜¯å‘ç—…5å¤©ï¼Œè…¹ç—›å‡è½»ä½†ä»æœ‰å‘çƒ­ï¼Œå¯èƒ½å¤„äºé˜‘å°¾å‘¨å›´è„“è‚¿çš„é˜¶æ®µã€‚è¿™æ—¶å€™å¦‚æœæ‚£è€…ä¸€èˆ¬æƒ…å†µç¨³å®šï¼Œå¯ä»¥å…ˆç”¨æŠ—ç”Ÿç´ æ²»ç–—ï¼ŒåŒæ—¶è§‚å¯Ÿæ˜¯å¦æœ‰è„“è‚¿å½¢æˆã€‚å¦‚æœåŒ…å—æŒç»­å­˜åœ¨æˆ–è€…æ„ŸæŸ“æ§åˆ¶ä¸ä½³ï¼Œå¯èƒ½éœ€è¦æ‰‹æœ¯å¼•æµæˆ–è€…åˆ‡é™¤ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œè¿˜è¦è€ƒè™‘æ‚£è€…çš„æ•´ä½“çŠ¶å†µï¼Œæ¯”å¦‚æ˜¯å¦æœ‰å…¶ä»–å¹¶å‘ç—‡ï¼Œå¦‚è…¹è†œç‚çš„è¿¹è±¡ï¼Œæˆ–è€…æ˜¯å¦å­˜åœ¨å…¶ä»–æ„ŸæŸ“æºã€‚åŒæ—¶ï¼Œå½±åƒå­¦æ£€æŸ¥å¦‚è¶…å£°æˆ–CTå¯èƒ½æœ‰åŠ©äºæ˜ç¡®è¯Šæ–­ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰è„“è‚¿å½¢æˆæˆ–é˜‘å°¾ç©¿å­”ã€‚\n",
      "\n",
      "æ‰€ä»¥å¤„ç†æ­¥éª¤å¯èƒ½åŒ…æ‹¬ï¼š1. è¿›è¡Œå½±åƒå­¦æ£€æŸ¥ç¡®è®¤æ˜¯å¦æœ‰è„“è‚¿ï¼›2. å¼€å§‹å¹¿è°±æŠ—ç”Ÿç´ æ²»ç–—ï¼›3. æ ¹æ®æ£€æŸ¥ç»“æœå†³å®šæ˜¯å¦éœ€è¦æ‰‹æœ¯å¼•æµæˆ–åˆ‡é™¤ã€‚å¦‚æœæ‚£è€…æƒ…å†µç¨³å®šï¼Œå¯ä»¥å…ˆä¿å®ˆæ²»ç–—ï¼Œå¦åˆ™éœ€è¦æ‰‹æœ¯å¹²é¢„ã€‚\n",
      "</think>\n",
      "\n",
      "å¯¹äºè¯¥æ‚£è€…ï¼Œå¤„ç†éœ€éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š\n",
      "\n",
      "**1. æ˜ç¡®è¯Šæ–­ä¸è¯„ä¼°å¹¶å‘ç—‡**\n",
      "- **ç—…ç¨‹åˆ†æ**ï¼šæ€¥æ€§é˜‘å°¾ç‚å‘ç—…5å¤©åè…¹ç—›å‡è½»ä½†æŒç»­å‘çƒ­ï¼Œæç¤ºå¯èƒ½è¿›å…¥**é˜‘å°¾å‘¨å›´è„“è‚¿å½¢æˆæœŸ**ï¼ˆå³é˜‘å°¾ç©¿å­”åå±€éƒ¨æ„ŸæŸ“å±€é™åŒ–é˜¶æ®µï¼‰ã€‚å³ä¸‹è…¹å‹ç—›æ€§åŒ…å—è¿›ä¸€æ­¥æ”¯æŒæ­¤è¯Šæ–­ã€‚\n",
      "- **é‰´åˆ«è¯Šæ–­**ï¼šéœ€æ’é™¤å…¶ä»–å¯èƒ½ï¼ˆå¦‚è‚ ç»“æ ¸ã€å¦‡ç§‘ç–¾ç—…ç­‰ï¼‰ï¼Œä½†ç»“åˆå…¸å‹ç—…å²å’Œä½“å¾ï¼Œè„“è‚¿å¯èƒ½æ€§æœ€å¤§ã€‚\n",
      "\n",
      "**2. å½±åƒå­¦æ£€æŸ¥**\n",
      "- **é¦–é€‰è¶…å£°æˆ–CT**ï¼šç¡®è®¤å³ä¸‹è…¹æ˜¯å¦å­˜åœ¨**é˜‘å°¾å‘¨å›´è„“è‚¿**ï¼ˆå¦‚è„“æ¶²ç§¯èšã€å‘¨å›´è„‚è‚ªé—´éš™æ¨¡ç³Šç­‰ï¼‰ï¼Œæ˜ç¡®è„“è‚¿èŒƒå›´åŠæ˜¯å¦ç´¯åŠé‚»è¿‘å™¨å®˜ï¼ˆå¦‚è¾“å°¿ç®¡ã€è†€èƒ±ï¼‰ã€‚\n",
      "\n",
      "**3. æ²»ç–—ç­–ç•¥**\n",
      "- **åˆå§‹æ²»ç–—ï¼šæŠ—ç”Ÿç´ æ§åˆ¶æ„ŸæŸ“**\n",
      "  - é€‰æ‹©**å¹¿è°±æŠ—ç”Ÿç´ **ï¼ˆå¦‚å¤´å­¢ç±»è”åˆç”²ç¡å”‘ï¼‰ï¼Œè¦†ç›–é©å…°æ°é˜´æ€§èŒå’ŒåŒæ°§èŒã€‚\n",
      "  - è‹¥æ‚£è€…å­˜åœ¨è„“æ¯’ç—‡å¾è±¡ï¼ˆå¦‚é«˜çƒ­ã€ç™½ç»†èƒæ˜¾è‘—å‡é«˜ï¼‰ï¼Œéœ€é™è„‰ç»™è¯ã€‚\n",
      "- **è„“è‚¿å¤„ç†**\n",
      "  - **ä¿å®ˆæ²»ç–—**ï¼šè‹¥è„“è‚¿è¾ƒå°ï¼ˆ<5cmï¼‰ã€æ‚£è€…ä¸€èˆ¬çŠ¶å†µè‰¯å¥½ï¼Œå¯ç»§ç»­æŠ—ç”Ÿç´ æ²»ç–—ï¼Œè§‚å¯Ÿè„“è‚¿æ˜¯å¦è‡ªè¡Œå¸æ”¶ã€‚\n",
      "  - **æ‰‹æœ¯å¹²é¢„**ï¼šè‹¥è„“è‚¿è¾ƒå¤§ï¼ˆ>5cmï¼‰ã€æŒç»­å‘çƒ­æˆ–æ„ŸæŸ“æ§åˆ¶ä¸ä½³ï¼Œéœ€è¡Œ**ç»çš®ç©¿åˆºå¼•æµ**ï¼ˆè¶…å£°å¼•å¯¼ä¸‹ï¼‰æˆ–**è…¹è…”é•œå¼•æµ**ã€‚è‹¥æ‚£è€…ç—…æƒ…ç¨³å®šï¼Œå¯åœ¨æ„ŸæŸ“æ§åˆ¶å2-4å‘¨è¡Œ**é˜‘å°¾åˆ‡é™¤æœ¯**ï¼ˆé¿å…å¤å‘ï¼‰ã€‚\n",
      "\n",
      "**4. ç›‘æµ‹ä¸éšè®¿**\n",
      "- å¯†åˆ‡è§‚å¯Ÿä½“æ¸©ã€ç™½ç»†èƒè®¡æ•°åŠå±€éƒ¨ä½“å¾å˜åŒ–ã€‚\n",
      "- è‹¥è„“è‚¿æœªæ¶ˆé€€æˆ–å‡ºç°å¹¶å‘ç—‡ï¼ˆå¦‚è„“æ¯’ç—‡ã€è‚ æ¢—é˜»ï¼‰ï¼Œéœ€åŠæ—¶æ‰‹æœ¯ã€‚\n",
      "\n",
      "**5. ç‰¹æ®Šæ³¨æ„äº‹é¡¹**\n",
      "- **é¿å…è¿‡æ—©æ‰‹æœ¯**ï¼šè‹¥æœªæ˜ç¡®è„“è‚¿å½¢æˆï¼Œè¿‡æ—©æ‰‹æœ¯å¯èƒ½å¢åŠ æ„ŸæŸ“æ‰©æ•£é£é™©ã€‚\n",
      "- **å¦Šå¨ æ‚£è€…**ï¼šè‹¥ä¸ºå¥³æ€§éœ€æ’é™¤å¼‚ä½å¦Šå¨ ï¼Œè¶…å£°æ£€æŸ¥éœ€æ³¨æ„å­å®«åŠé™„ä»¶æƒ…å†µã€‚\n",
      "\n",
      "**æ€»ç»“**ï¼šå½“å‰åº”å…ˆè¿›è¡Œå½±åƒå­¦æ£€æŸ¥æ˜ç¡®è„“è‚¿å­˜åœ¨ï¼Œéšåä»¥æŠ—ç”Ÿç´ æ²»ç–—ä¸ºä¸»ï¼Œæ ¹æ®è„“è‚¿å¤§å°åŠæ„ŸæŸ“æ§åˆ¶æƒ…å†µå†³å®šæ˜¯å¦éœ€å¼•æµæˆ–å»¶æœŸæ‰‹æœ¯ã€‚<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff02fb-936a-46f8-986c-1f056e2d8e11",
   "metadata": {},
   "source": [
    "## å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20fce5aa-0b13-4beb-ae83-dd177a93c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Please answer the following medical question. \n",
    "### Question:\n",
    "{}\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think> \n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ab19ed-69d9-4522-8cc0-b5bcbda9e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.0\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "275bf519-674a-4e94-9f4f-cf49ebc63c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 08:45:26,496 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from medical-o1-reasoning-SFT. Please make sure that you can trust the external codes.\n",
      "2025-06-16 08:45:26,926 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from FreedomIntelligence/medical-o1-reasoning-SFT. Please make sure that you can trust the external codes.\n",
      "2025-06-16 08:45:26,928 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from FreedomIntelligence/medical-o1-reasoning-SFT. Please make sure that you can trust the external codes.\n",
      "2025-06-16 08:45:26,928 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from FreedomIntelligence/medical-o1-reasoning-SFT. Please make sure that you can trust the external codes.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "dataset=MsDataset.load(\n",
    "    dataset_name='medical-o1-reasoning-SFT',\n",
    "    namespace='FreedomIntelligence',\n",
    "    subset_name='zh',  # æ­£ç¡®å‚æ•°å\n",
    "    split='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ec2e850-8b75-431d-8cc5-da1a3338e469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Question', 'Response', 'Complex_CoT'],\n",
       "    num_rows: 20171\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc5fd693-58b1-4879-9da0-64735b4aac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # æœ«å°¾å¿…é¡»åŠ ä¸Š EOS_TOKEN\n",
    " \n",
    " \n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f334926-9e5d-49cd-87cc-c634dc24db35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Question', 'Response', 'Complex_CoT', 'text'],\n",
       "    num_rows: 20171\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1dc13cb-b1fd-4475-9756-4faec386782c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. \n",
      "Write a response that appropriately completes the request. \n",
      "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
      "### Instruction:\n",
      "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
      "Please answer the following medical question. \n",
      "### Question:\n",
      "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\n",
      "### Response:\n",
      "<think>\n",
      "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
      "\n",
      "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
      "\n",
      "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
      "\n",
      "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
      "\n",
      "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
      "\n",
      "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
      "\n",
      "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
      "\n",
      "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
      "\n",
      "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\n",
      "</think> \n",
      "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b373bfe-e57d-41a9-b27c-fc06b48917d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(model,r=16,target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],lora_alpha=16,lora_dropout=0,bias=\"none\",use_gradient_checkpointing=\"unsloth\",random_state=3407,use_rslora=False,loftq_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f2261ed-18c8-4ae4-b7de-34b112e308b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer \n",
    "from transformers import TrainingArguments \n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "810953db-3d1e-43ab-8e7e-b38510522709",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "_flag_for_generation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_num_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_bfloat16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbf16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_bfloat16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3407\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/trainer.py:210\u001b[0m, in \u001b[0;36m_backwards_compatible_trainer.<locals>.new_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/fintuning/unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py:1087\u001b[0m, in \u001b[0;36mUnslothSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1085\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 1087\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding_side\u001b[39m\u001b[38;5;124m'\u001b[39m): tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessing_class\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:2792\u001b[0m, in \u001b[0;36mFastLlamaModel.for_training\u001b[0;34m(model, use_gradient_checkpointing)\u001b[0m\n\u001b[1;32m   2790\u001b[0m m \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 2792\u001b[0m     \u001b[43m_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2793\u001b[0m     m \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   2794\u001b[0m _for_training(m)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:2788\u001b[0m, in \u001b[0;36mFastLlamaModel.for_training.<locals>._for_training\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_saved_temp_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m): m\u001b[38;5;241m.\u001b[39m_saved_temp_tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;66;03m# Set a flag for generation!\u001b[39;00m\n\u001b[0;32m-> 2788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flag_for_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;28;01mdel\u001b[39;00m m\u001b[38;5;241m.\u001b[39m_flag_for_generation\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:2052\u001b[0m, in \u001b[0;36mModule.__delattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[name]\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__delattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: _flag_for_generation"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=64,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs = 20, \n",
    "        warmup_steps=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8271038-e139-4e00-9bf5-1fc03a4b6304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 20 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 512 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (512 x 4 x 1) = 2,048\n",
      " \"-____-\"     Trainable parameters = 64,225,280/14,832,532,480 (0.43% trained)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 723.81 MiB is free. Process 1966371 has 30.71 GiB memory in use. Process 1035366 has 18.96 GiB memory in use. Including non-PyTorch memory, this process has 44.59 GiB memory in use. Of the allocated memory 44.06 GiB is allocated by PyTorch, and 27.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/zhangdw/workspace/fintuning/unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py:846\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m--> 846\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/zhangdw/workspace/fintuning/unsloth/unsloth_compiled_cache/UnslothSFTTrainer.py:835\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 835\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/_utils.py:1069\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1067\u001b[0m     )\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/transformers/trainer.py:3810\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3809\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3810\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/accelerate/utils/operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/accelerate/utils/operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:1260\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1259\u001b[0m ):\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:1103\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1103\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:924\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/transformers/modeling_layers.py:47\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/utils/checkpoint.py:488\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m         )\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    491\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    492\u001b[0m     )\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth_zoo/gradient_checkpointing.py:475\u001b[0m, in \u001b[0;36mUnslothCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39m_requires_gradient: ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 475\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM\u001b[38;5;241m.\u001b[39mwait_stream(EXTRA_STREAM)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/llama.py:592\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 592\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/models/qwen3.py:89\u001b[0m, in \u001b[0;36mQwen3Attention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m head_dim   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(n_kv_heads \u001b[38;5;241m*\u001b[39m n_groups \u001b[38;5;241m==\u001b[39m n_heads)\n\u001b[0;32m---> 89\u001b[0m Q, K, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m Q \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mview(bsz, q_len, n_heads,    head_dim)\u001b[38;5;66;03m#.transpose(1, 2) # we will transpose after normalisation\u001b[39;00m\n\u001b[1;32m     91\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mview(bsz, q_len, n_kv_heads, head_dim)\u001b[38;5;66;03m#.transpose(1, 2) # we will transpose after normalisation\u001b[39;00m\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:366\u001b[0m, in \u001b[0;36mapply_lora_qkv\u001b[0;34m(self, X, inplace)\u001b[0m\n\u001b[1;32m    364\u001b[0m KW, KW_quant, KA, KB, KS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj)\n\u001b[1;32m    365\u001b[0m VW, VW_quant, VA, VB, VS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj)\n\u001b[0;32m--> 366\u001b[0m Q, K, V \u001b[38;5;241m=\u001b[39m \u001b[43mLoRA_QKV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q, K, V\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/torch/amp/autocast_mode.py:510\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:261\u001b[0m, in \u001b[0;36mLoRA_QKV.forward\u001b[0;34m(ctx, X, QW, QW_quant, QA, QB, QS, KW, KW_quant, KA, KB, KS, VW, VW_quant, VA, VB, VS, inplace)\u001b[0m\n\u001b[1;32m    259\u001b[0m Q \u001b[38;5;241m=\u001b[39m matmul_lora(X, QW, QW_quant, QA, QB, QS)\n\u001b[1;32m    260\u001b[0m K \u001b[38;5;241m=\u001b[39m matmul_lora(X, KW, KW_quant, KA, KB, KS)\n\u001b[0;32m--> 261\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m ctx\u001b[38;5;241m.\u001b[39mcustom_saved_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    264\u001b[0m     QW, QW_quant, QS,\n\u001b[1;32m    265\u001b[0m     KW, KW_quant, KS,\n\u001b[1;32m    266\u001b[0m     VW, VW_quant, VS,\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    268\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(X, QA, QB, KA, KB, VA, VB,)\n",
      "File \u001b[0;32m~/zhangdw/workspace/uv/unsloth/lib/python3.10/site-packages/unsloth/kernels/utils.py:485\u001b[0m, in \u001b[0;36mmatmul_lora\u001b[0;34m(X, W, W_quant, A, B, s, out)\u001b[0m\n\u001b[1;32m    483\u001b[0m     reshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m W_quant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mdel\u001b[39;00m W\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# LoRA is enabled\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 723.81 MiB is free. Process 1966371 has 30.71 GiB memory in use. Process 1035366 has 18.96 GiB memory in use. Including non-PyTorch memory, this process has 44.59 GiB memory in use. Of the allocated memory 44.06 GiB is allocated by PyTorch, and 27.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfd444-a035-4e82-ae81-aa2de96365ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
